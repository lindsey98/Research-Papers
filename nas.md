
Neural Architecture search is a subset of Hyperparameter Optimization problem. As when we talking about hyperparameter, it includes architectural param (num of layers, num of nodes per layer, type of activation used) and non-architectural param (learning rate, trade-off parameter).

## Hyperparameter Optimization

### Definition: 
Algorithm is A, \lambda is the hyperparameter(s), a HPO problem is trying to ![equation](https://latex.codecogs.com/gif.latex?%5Clambda*%20%3D%20argmin_%7B%5Clambda%7D%7BL%28A_%7B%5Clambda%7D%2C%20D_%7B%5Ctextit%7Btrain%7D%7D%2C%20D_%7B%5Ctextit%7Bvalid%7D%7D%20%29%7D)

### Blackbox optimization
Given lambda, feed into a black-box model, get f(\lambda)

### Methods
- Grid search or Random search









## NAS
